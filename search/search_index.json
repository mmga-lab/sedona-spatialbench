{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sedona SpatialBench","text":"<p>SpatialBench is a benchmark for assessing geospatial SQL analytics query performance across database systems.</p> <p>SpatialBench makes it easy to run spatial benchmarks on a realistic dataset with any query engine.</p> <p>The methodology is unbiased and the benchmarks in any environment to compare relative performance between runtimes.</p>"},{"location":"#why-spatialbench","title":"Why SpatialBench","text":"<p>SpatialBench is a geospatial benchmark for testing and optimizing spatial analytical query performance in database systems. Inspired by the SSB and NYC taxi data, it combines realistic urban mobility scenarios with a star schema extended with spatial attributes like pickup/dropoff points, zones, and building footprints.</p> <p>This design enables evaluation of the following geospatial operations:</p> <ul> <li>spatial joins</li> <li>distance queries</li> <li>aggregations</li> <li>point-in-polygon analysis</li> </ul> <p>Let\u2019s dive into the advantages of SpatialBench.</p>"},{"location":"#key-advantages","title":"Key advantages","text":"<ul> <li>Uses spatial datasets with geometry columns.</li> <li>Includes queries with different spatial predicates.</li> <li>Easily reproducible results.</li> <li>Includes a dataset generator to so results are reproducible.</li> <li>The scale factors of the datasets can be changed so that you can run the queries locally, in a data warehouse, or on a large cluster in the cloud.</li> <li>All the specifications used to run the benchmarks are documented, and the methodology is unbiased.</li> <li>The code is open source, allowing the community to provide feedback and keep the benchmarks up-to-date and reliable over time.</li> </ul>"},{"location":"#generate-synthetic-data","title":"Generate synthetic data","text":"<p>Here\u2019s how you can install the synthetic data generator:</p> <pre><code>cargo install --path ./spatialbench-cli\n</code></pre> <p>Here\u2019s how you can generate the synthetic dataset:</p> <pre><code>spatialbench-cli -s 1 --format=parquet\n</code></pre> <p>See the project repository README for the complete set of straightforward data generation instructions.</p>"},{"location":"#example-query","title":"Example query","text":"<p>Here\u2019s an example query that counts the number of trips that start within 500 meters of each building:</p> <pre><code>SELECT\n    b.b_buildingkey,\n    b.b_name,\n    COUNT(*) AS nearby_pickup_count\nFROM trip t\nJOIN building b\nON ST_DWithin(t.t_pickup_loc, b.b_boundary, 500)\nGROUP BY b.b_buildingkey, b.b_name\nORDER BY nearby_pickup_count DESC;\n</code></pre> <p>This query performs a distance join, followed by an aggregation.  It\u2019s a great example of a query that\u2019s useful for performance benchmarking a spatial engine that can process vector geometries.</p>"},{"location":"#join-the-community","title":"Join the community","text":"<p>Feel free to start a GitHub Discussion or join the Discord community to ask the developers any questions you may have.</p> <p>We look forward to collaborating with you on these benchmarks!</p>"},{"location":"datasets-generators/","title":"SpatialBench Datasets and Generators","text":""},{"location":"datasets-generators/#spatialbench-datasets-and-generators","title":"SpatialBench Datasets and Generators","text":"<p>This page describes the SpatialBench datasets and shows you how to use the generators to create the spatial tables.</p> <p>SpatialBench is a geospatial benchmark designed for evaluating and optimizing spatial query performance in data systems. Inspired by the Star Schema Benchmark (SSB) and the New York City Taxi and Limousine Commission (NYC TLC) dataset, SpatialBench blends realistic urban mobility scenarios with standardized benchmarking practices.</p> <p>The benchmark adopts the familiar star schema structure from SSB, augmented with spatial attributes such as pickup and dropoff points, spatial polygon boundaries for zones, and building footprints. These spatial enhancements allow SpatialBench to effectively test geospatial operations, including spatial joins, distance-based queries, spatial aggregations, and point-in-polygon analyses.</p> <p>By combining the systematic approach of SSB with authentic, real-world scenarios drawn from NYC TLC data, SpatialBench provides meaningful and practical benchmarks relevant to urban mobility and spatial analytics workloads.</p>"},{"location":"datasets-generators/#data-model","title":"Data model","text":"<p>SpatialBench tables:</p> <ul> <li>Trip (Fact Table): Records individual trips, including spatial attributes (pickup and dropoff points), trip fare, distance, duration, and timestamps for pickup and dropoff.</li> <li>Customer: Represents customers who book trips.</li> <li>Driver: Represents drivers who fulfill trips.</li> <li>Vehicle: Details about vehicles used for trips.</li> <li>Zone: Polygon boundaries representing city areas or zones.</li> <li>Building: Polygon footprints representing building locations, types, and names.</li> </ul> Table Type Abbr. Primary Role Spatial Attributes Size per Scale Factor (SF) Building Dimension b_ Polygon footprints representing building locations Polygon footprints 20K \u00d7 (1 + log\u2082(SF)) Customer Dimension c_ Represents customers None 30K \u00d7 SF Driver Dimension s_ Represents drivers None 500 x SF Trip Fact Table t_ Records individual trips Pickup/Dropoff Points (location) 6M \u00d7 SF Vehicle Dimension v_ Details about vehicles None 100 x SF Zone Dimension z_ Polygon boundaries for city zones Polygon boundaries Tiered by SF range (see below)"},{"location":"datasets-generators/#zone-table-scaling","title":"Zone Table Scaling","text":"Scale Factor (SF) Zone Subtypes Included Zone Cardinality [0, 10) microhood, macrohood, county 156,095 [10, 100) + neighborhood 455,711 [100, 1000) + localadmin, locality, region, dependency 1,035,371 [1000+) + country 1,035,749"},{"location":"datasets-generators/#geographic-coverage","title":"Geographic Coverage","text":"<p>Spatial Bench's data generator uses continent-bounded affines. Each continent is defined by a bounding polygon, ensuring generation mostly covers land areas and introducing the natural skew of real geographies.</p> <p>Bounding polygons:</p> Region Bounding Polygon Africa <code>POLYGON ((-20.062752 -40.044425, 64.131567 -40.044425, 64.131567 37.579421, -20.062752 37.579421, -20.062752 -40.044425))</code> Europe <code>POLYGON ((-11.964479 37.926872, 64.144374 37.926872, 64.144374 71.82884, -11.964479 71.82884, -11.964479 37.926872))</code> South Asia <code>POLYGON ((64.58354 -9.709049, 145.526096 -9.709049, 145.526096 51.672557, 64.58354 51.672557, 64.58354 -9.709049))</code> North Asia <code>POLYGON ((64.495655 51.944267, 178.834704 51.944267, 178.834704 77.897255, 64.495655 77.897255, 64.495655 51.944267))</code> Oceania <code>POLYGON ((112.481901 -48.980212, 180.768942 -48.980212, 180.768942 -10.228433, 112.481901 -10.228433, 112.481901 -48.980212))</code> South America <code>POLYGON ((-83.833822 -56.170016, -33.904338 -56.170016, -33.904338 12.211188, -83.833822 12.211188, -83.833822 -56.170016))</code> South North America <code>POLYGON ((-124.890724 12.382931, -69.511192 12.382931, -69.511192 42.55308, -124.890724 42.55308, -124.890724 12.382931))</code> North North America <code>POLYGON ((-166.478008 42.681087, -52.053245 42.681087, -52.053245 72.659041, -166.478008 72.659041, -166.478008 42.681087))</code> <p></p>"},{"location":"datasets-generators/#distribution-options","title":"Distribution Options","text":"<p>By default, SpatialBench generates points using continent-bounded affines with a Hierarchical Thomas distribution for the trip and building tables.  </p> <p>For more realism, you can choose from a variety of spatial distributions when generating tables:</p> <ul> <li>Uniform: Evenly spread points in the unit square.  </li> <li>Normal: Gaussian spread around a mean with configurable variance.  </li> <li>Diagonal: Points concentrated along the y=x diagonal with configurable buffer.  </li> <li>Bit: Recursive grid-like pattern controlled by probability and bit depth.  </li> <li>Sierpinski: Self-similar fractal pattern for highly skewed coverage.  </li> <li>Thomas: Clustered distribution with realistic hotspots and heavy-tailed skew.  </li> <li>Hierarchical Thomas: Multi-level clustering (cities \u2192 neighborhoods \u2192 points), useful for mimicking urban settlement patterns.</li> </ul> <p>These options let you tailor the spatial skew to your benchmarking needs.  </p> <p>See the distributions page to learn more about the supported spatial distributions, the parameters that control them, and how they impact the data.</p>"},{"location":"datasets-generators/#data-generators","title":"Data generators","text":"<p>You can generate the tables for Scale Factor 1 with the following command:</p> <pre><code>spatialbench-cli -s 1 --format=parquet --output-dir sf1-parquet\n</code></pre> <p>Here are the contents of the <code>sf1-parquet</code> directory:</p> <ul> <li><code>building.parquet</code></li> <li><code>customer.parquet</code></li> <li><code>driver.parquet</code></li> <li><code>trip.parquet</code></li> <li><code>vehicle.parquet</code></li> <li><code>zone.parquet</code></li> </ul> <p>See the README for a full description of how to use the SpatialBench data generators.</p>"},{"location":"overview-methodology/","title":"SpatialBench Overview and Methodology","text":""},{"location":"overview-methodology/#spatialbench-overview-and-methodology","title":"SpatialBench Overview and Methodology","text":"<p>SpatialBench is an open benchmark suite of representative spatial queries designed to evaluate the performance of different engines at multiple scale factors.</p> <p>The SpatialBench queries are a great way to compare the relative performance between engines for analytical spatial workloads.  You can use a small scale factor for single-machine queries, and a large scale factor to benchmark an engine that distributes computations in the cloud.</p> <p>Let\u2019s take a deeper look at why SpatialBench is so essential.</p>"},{"location":"overview-methodology/#why-spatialbench","title":"Why SpatialBench?","text":"<p>Spatial workflows encompass queries such as spatial joins, spatial filtering, and spatial-specific operations, including KNN joins.</p> <p>General-purpose analytics query benchmarks don\u2019t cover spatial queries.  They focus on analytical queries, such as joins and aggregations, on tabular data. Here are some popular analytical benchmarks:</p> <ul> <li>TPC-H</li> <li>TPC-DS</li> <li>ClickBench</li> <li>YCSB</li> <li>db-benchmark</li> </ul> <p>The analytical benchmarks help analyze analytical performance, but that doesn\u2019t necessarily translate to spatial queries.  An engine can be blazing fast for a large tabular aggregation and terrible for spatial joins.</p> <p>SpatialBench is tailored for spatial queries.  It\u2019s the best modern option to assess the spatial performance of an engine.  Let\u2019s take a look at some of the older spatial benchmarks.</p>"},{"location":"overview-methodology/#hardware-and-software","title":"Hardware and software","text":"<p>SpatialBench runs benchmarks on commodity hardware, with software versions fully disclosed for each release.</p> <p>When comparing different runtimes, developers should make a good-faith effort to use similar hardware and software versions.  It\u2019s not helpful to compare one runtime with another runtime that has a lot less computational power.</p> <p>SpatialBench benchmarks should always be presented with associated hardware/software specifications so readers can assess the reliability of the comparison.</p>"},{"location":"overview-methodology/#accurately-comparing-different-engines","title":"Accurately comparing different engines","text":"<p>It is challenging to compare fundamentally different engines, such as PostGIS (an OLTP database), DuckDB (an OLAP database), and GeoPandas (a Python engine).</p> <p>For example, let\u2019s compare how two engines execute a query differently:</p> <ul> <li>PostGIS: create tables, load data into the tables, build an index (can be expensive), run the query</li> <li>GeoPandas: read data into memory and run a query</li> </ul> <p>PostGIS and GeoPandas execute queries differently, so you need to present the query runtime with caution.  For example, you can\u2019t just ignore the time it takes to build the PostGIS index because that can be the slowest part of the query.  That\u2019s a critical detail for users running ad hoc queries.</p> <p>The SpatialBench results strive to present runtimes for all relevant portions of the query so users are best informed about how to interpret the results.</p>"},{"location":"overview-methodology/#engine-tuning-in-benchmarks","title":"Engine tuning in benchmarks","text":"<p>Engines can be tuned by configuring settings or optimizing code.  For example, you can optimize Spark code by tuning the JVM.  You can optimize GeoPandas code by adding indexes.  Benchmarks that tune one engine and don\u2019t tune any of the other engines aren\u2019t reliable.</p> <p>All performance tuning is fully disclosed in the SpatialBench results.  Some results are presented both naively and fully tuned to give a better picture of out-of-the-box performance and what\u2019s possible for expert users.</p>"},{"location":"overview-methodology/#open-source-benchmarks-vs-vendor-benchmarks","title":"Open source benchmarks vs. vendor benchmarks","text":"<p>The SpatialBench benchmarks report results for some open source spatial engines/databases.</p> <p>The SpatialBench repository does not report results for any proprietary engines or vendor runtimes.  Vendors are free to use the SpatialBench data generators and run the benchmarks on their own.  We ask vendors to credit SpatialBench when they run the benchmarks and fully disclose the results so that other practitioners can reproduce the results.</p>"},{"location":"overview-methodology/#how-to-contribute","title":"How to contribute","text":"<p>There are a variety of ways to contribute to the SpatialBench project:</p> <ul> <li>Submit pull requests to add features</li> <li>Create issues for bug reports</li> <li>Reproduce results or help add new spatial engines</li> <li>Publish vendor benchmarks</li> </ul> <p>Here is how you can communicate with the team:</p> <ul> <li>Chat with us on the Apache Sedona Discord</li> <li>Create GitHub Discussions</li> </ul>"},{"location":"overview-methodology/#future-work","title":"Future work","text":"<p>In the next release, we will add raster datasets and raster queries.  These will stress test an engine\u2019s ability to analyze raster data.  They will also show performance when joining vector and raster datasets.</p>"}]}